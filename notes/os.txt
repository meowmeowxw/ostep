What happens when a program runs?

-------------------------------------------------------------------------------
Too many things.. :(
-------------------------------------------------------------------------------

It does one very simple thing: it executes instruction. To do that the CPU
fetches an instruction from memmory, decodes it and exectues it. The CPU
repeats this cycle until the program completes.

However when a program runs, a lot of other things are going on.

There is a body of software, in fact, that is responsible for making it easy
to run programs, allowing programs to share memory, enabling programs to
interact with devices, and other fun stuff like that. This system is called
the operating system (OS).

The primary way the OS does this is through a general technique that we
call virtualization. That is, the OS takes a physical resource (such as
the processor, or memory, or a disk) and transforms it into a more general,
powerful, and easy--to-use virtual form of itself.

The OS also provides some interfaces (APIs) that you can call.  A typical OS,
in fact, exports a few hundred system calls that are available to applications.

Turning a single CPU (or small set of them) into a seemingly infinite number
of CPUs and thus allowing many programs to seemingly run at once is what we
call virtualizing the CPU.

Each process accesses its own private virtual address space, which the OS
somehow maps onto the physical memory of the machine.  A memory reference
within one running program does not affect the address space of other processes
(or the OS itself); as far as the running program is concerned, it has
physical memory all to itself (Virtualizing memory).  The reality, however,
is that physical memory is a shared resource, managed by the operating system.

-------------------------------------------------------------------------------
Process
-------------------------------------------------------------------------------

The process is the major OS abstraction of a running program. At any point
in time, the process can be described by its state: the contents of memory
in its address space, the contents of CPU registers (including the program
counter and stack pointer, among others), and information about I/O (such
as open files which can be read or written).

The process API consists of calls programs can make related to processes.
Typically, this includes creation, destruction, and other useful calls.

Processes exist in one of many different process states, including running,
ready to run, and blocked. Different events (e.g., getting scheduled or
descheduled, or waiting for an I/O to complete) transition a process from
one of these states to the other.

A process list contains information about all processes in the sys-- tem. Each
entry is found in what is sometimes called a process control block (PCB), which
is really just a structure that contains information about a specific process.

-------------------------------------------------------------------------------
Process API
-------------------------------------------------------------------------------

Each process has a name; in most systems, that name is a number known as a
process ID (PID).

The fork() system call is used in UNIX systems to create a new process.
The creator is called the parent; the newly created process is called the
child. As sometimes occurs in real life [J16], the child process is a nearly
identical copy of the parent.

The wait() system call allows a parent to wait for its child to complete
execution.

The exec() family of system calls allows a child to break free from its
similarity to its parent and execute an entirely new program.

A U NIX shell commonly uses fork(), wait(), and exec() to launch user
commands; the separation of fork and exec enables features like input/output
redirection, pipes, and other cool features, all without changing anything
about the programs being run.

Process control is available in the form of signals, which can cause jobs
to stop, continue, or even terminate.

Which processes can be controlled by a particular person is encapsulated in
the notion of a user; the operating system allows multiple users onto the
system, and ensures users can only control their own processes.

A superuser can control all processes (and indeed do many other things);
this role should be assumed infrequently and with caution for security reasons.

-------------------------------------------------------------------------------
Limited Direct Execution
-------------------------------------------------------------------------------

The CPU should support at least two modes of execution: a restricted user
mode and a privileged (non-restricted) kernel mode.

Typical user applications run in user mode (In this way the process can't
issue I/O directly to the disk or other privileged actions), and use a system
call to trap into the kernel to request operating system services.

The trap instruction saves register state carefully, changes the hardware
status to kernel mode, and jumps into the OS to a pre-specified destination:
the trap table.

When the OS finishes servicing a system call, it returns to the user program
via another special return-from-trap instruction, which reduces privilege
and returns control to the instruction after the trap that jumped into the OS.

The trap tables must be set up by the OS at boot time, and make sure that
they cannot be readily modified by user programs. All of this is part of
the limited direct execution protocol which runs programs efficiently but
without loss of OS control.

Once a program is running, the OS must use hardware mechanisms to ensure the
user program does not run forever, namely the timer interrupt. This approach
is a non-cooperative approach to CPU scheduling.

Sometimes the OS, during a timer interrupt or system call, might wish to
switch from running the current process to a different one, a low-level
technique known as a context switch. This decision is taken by the scheduler
using a scheduling policy.


-------------------------------------------------------------------------------
Scheduling
-------------------------------------------------------------------------------

The first thing we need to do is to introduce the assumptions about the
processes running in the system (workload). Possible workload assumptions:

1. Each job runs for the same amount of time.  2. All jobs arrive at the
same time.  3. Once started, each job runs to completion.  4. All jobs only
use the CPU (i.e., they perform no I/O) 5. The run-time of each job is known.

To measure how well a scheduling policy behave we need to define the scheduling
metric. The easiest metric is the turnaround time.  The turnaround time of
a job is defined as the time at which the job completes minus the time at
which the job arrived in the system:

T_turnaround = T_completion - T_arrival

------- FIFO -------

Let's start using FIFO (First-In-First-Out) policy.

Assume that a job completes in 10 seconds, and job A, B, C arrive in the
system at roughly the same time (T_arrival = 0).

Job A (0 -> 10) 
Job B (0 -> 20) 
Job C (0 -> 30)

In this case average T_turnaround = (10 + 20 + 30) / 3 = 20 s, which is a
good time.

Now let's relax assumption 1. and suppose that A runs 100 seconds, while B
and C for 10 seconds.

Job A (0 -> 100) 
Job B (0 -> 110) 
Job C (0 -> 120)

Average T_turnaround = (100 + 110 + 120) / 3 = 110 s, which is terrible.

------- SJF -------

SJF (Shortest Job First) run the shortest job first (wow), then the next
shortest, and so on. If we take the precedent case:

Job B (0 -> 10) 
Job C (0 -> 20) 
Job A (0 -> 120)

Average T_turnaround = (10 + 20 + 120) / 3 = 50 s which is way better
than FIFO.

Now let's relax assumption 2, and suppose that A arrives at t = 0, while B
and C arrives at t = 10 and both run for 10 seconds.

Job A (0 -> 100)
Job B (10 -> 110) 
Job C (10 -> 120)

Average T_turnaround = (100 + (110 - 10) + (120 - 10)) / 3 = 103.33 s.

------- STCF -------

TO address this concern, we need to relax assumption 3.

Using a STCF (Shortest Time to Completion First) policy the above case will
be scheduled as:

Job A (0 -> 10) 
Job B (10 -> 20) 
Job C (10 -> 30) 
Job A (30 -> 120)

Average T_turnaround = ((120 - 0) + (20 - 10) + (30 - 10)) / 3 = 50 s.

Thus, if we knew job lengths, and that jobs only used the CPU, and our
only metric was turnaround time, STCF would be a great policy. However, the
introduction of time-shared machines changed all that.  Now users would sit
at a terminal and demand interactive performance from the system as well. And
thus, a new metric was born: response time.

We define response time as the time from when job arrives to the system to
the first time is scheduled.

T_response = T_firstrun - T_arrival

If A has T_arrival = 0, B T_arrival = 10, C T_arrival = 10, and each job
lasts for 10 seconds then

Average T_response = (0 + 0 + 10) / 3 = 3.33 s

In this case STCF and SJF are not particularly good for response time.

------- RR -------

RR (Round Robin) don't wait for jobs to finish, instead it runs job for a
time slice and then switches to the next job in the run queue.

Assume A, B, C arrive at the same time in the system and each wish to run
for 3 seconds. If we set a time slice of 1 RR behave this way:

A (0 -> 1) 
B (1 -> 2) 
C (1 -> 3) 
A (3 -> 4) 
B (4 -> 5) 
C (5 -> 6) 
A (6 -> 7) 
B (7 -> 8) 
C (8 -> 9)

Average T_response = (0 + 1 + 2) / 3 = 1 s.

With SJF:

Average T_response = (0 + 3 + 6) / 3 = 3 s.

It sounds a good idea to set a very small time slice, however the cost of a
context switch is pretty big since the system must save/restore registers,
TLB, branch predictors and other on-chip hardware.

How good is turnaround time?

Average T_turnaround = (7 + 8 + 9) / 3 = 8 s which is awful.

We can see that there isn't a perfect policy for scheduling and we need to
trade the T_response for T_turnaround.

Now let's relax assumption 4.
A scheduler clearly has a decision to make when a job initiates an I/O
request, because the currently-running job won’t be using the CPU during the 
I/O; it is blocked waiting for I/O completion. If the I/O is sent to a hard 
disk drive, the process might be blocked for a few milliseconds or longer, 
depending on the current I/O load of the drive. Thus, the scheduler should 
probably schedule another job on the CPU at that time.

Suppose that we have two job A, B. A runs for 10 ms and then issue an I/O which
lasts for 10 ms and it repeat this procedure 3 times. B runs for 30 ms without
I/O.

If we use a normal SCTF and A runs first then:

A  (0 -> 10)
A' (10 -> 20) (I/O no CPU usage)
A  (20 -> 30)
A' (30 -> 40)
A  (40 -> 50)
A' (50 -> 60)
B  (60 -> 90)

This is not an efficient way to build a scheduler, if we overlap the jobs we
get this:

A  (0 -> 10)
A' (10 -> 20)
B  (10 -> 20)

A  (20 -> 30)
A' (30 -> 40)
B  (30 -> 40)

A  (40 -> 50)
A' (50 -> 60)
B  (50 -> 60)

Which is way more efficient.

Now relax assumption 5., which is the worst assumption we could make, since
usually we don't know the duration of each job. How can we build a scheduler
that behaves like STCF without a priori knowledge and incorporate some ideas
from RR to optimize response time?

------- MLFQ -------

The MLFQ (Multi Level Feedback Queue) is an approach that can learn and adapts
based on how jobs behave.

The MLFQ has a number of distinct queues, each assigned a different priority
level.
At any given time, a job that is ready to run is on a single queue. MLFQ uses 
priorities to decide which job should run at a given time: a job with higher 
priority (i.e., a job on higher queue) is chosen to run.

More than one job may be on a given queue, and thus have the same priority. 
In this case, we will just use Round-Robin scheduling among those jobs.

Thus, we arrive at the first two basic rules for MLFQ:

Rule 1: If Priority(A) > Priority(B), A runs (B doesn’t).
Rule 2: If Priority(A) = Priority(B), A & B run in RR.

MLFQ varies the priority of a job based on its observed behavior.

If a job relinquishes the CPU while waiting for input from the keyboard, MLFQ
will keep its priority high, as this is how an interactive process might 
behave. If, instead, a job uses the CPU intensively for long periods of time,
MLFQ will reduce its priority.

Example:

Q5 -> A -> B [High priority]
Q4
Q3 -> C
Q2 
Q1 -> D [Low priority]

However we need to consider that priority changes over time, and so the jobs
will be moved between various queues, so wee need to define how the priority 
changes with new rules:

Rule 3: When a job enters the system, it is placed at the highest priority.
Rule 4a: If a job uses up an entire time slice while running, its priority is
reduced.
Rule 4b: If a job gives up the CPU before the time slice is up, it stays at the
same priority level.

Rule 3 make the scheduler behaves similar to a SJF, we don't know the length of
each job so we place it at the top of the queue.
Rule 4a/b decides when a job need to move between queues. There's problem if a
job become interactive after being CPU consuming for a while it will remain at
the lowest priority, so:

Rule 5: After some time period S, move all the jobs in the system to the
topmost queue (boost).

However there's another problem, if a job gives up the CPU before the time 
slice is up each time, it can monopolize the CPU and game the scheduler.

Rule 4: Once a job uses up its time allotment at a given level (regardless of 
how many times it has given up the CPU), its priority is reduced.

In the Rule 5 we didn't define the value of S, since it's a voo-doo constants 
as the number of queues.

In some systems we can define suggests to the operating systems the priority
of jobs using the utility nice.

Many systems, including BSD UNIX derivatives, Solaris, and Windows NT and 
subsequent Windows operating systems use a form of MLFQ as their base scheduler.

------- CFS -------

CFS (Completely Fair Scheduler) is the linux kernel scheduler.

Whereas most schedulers are based around the concept of a fixed time slice, 
CFS operates a bit differently. Its goal is simple: to fairly divide a CPU 
evenly among all competing processes. It does so through a simple counting-based 
technique known as virtual runtime (vruntime).

As each process runs, it accumulates vruntime. In the most basic case, each 
process’s vruntime increases at the same rate, in proportion with physical
(real) time. When a scheduling decision occurs, CFS will pick the process with
the lowest vruntime to run next.

How does the scheduler know when to stop the currently running process, and run
the next one? If CFS switch too often, fairness is increased, as each job
receives its share of CPU even over miniscule time windows, but at the cost of 
performance. However if CFS switch less often, performance is increased but the
fairness is decreased.

CFS manages this tension through various control parameters. The first is 
sched_latency. CFS uses this value to determine how long one process one
process should run before considering a context switch. 
If sched_latency is set to 48 ms, and there are 4 running process, then CFS 
divides sched_latency by 4 to arrive at a per-process time slice of 12 ms.

If there are too many processes running in the system, the problem of the
frequents context switch remains, so CFS add a parameter, min_granularity
which is set to a value like 6 ms.

CFS also enables controls over process priority to the user with the utility 
nice. The nice parameter can be set anywhere from -20 to +19, with a default of
0. The lower the value the higher the priority (The nicer you are, the worst
you get). At each nice value is associated a weight as follow:

static const int prio_to_weight[40] = {
/* -20 */	88761, 71755, 56483, 46273, 36291,
/* -15 */	29154, 23254, 18705, 14949, 11916,
/* -10 */	9548, 7620, 6100, 4904, 3906,
/* -5 */	3121, 2501, 1991, 1586, 1277,
/* 0 */	1024, 820, 655, 526, 423, 
/* 5 */	335, 272, 215, 172, 137,
/* 10 */	110, 87, 70, 56, 45,
/* 15 */	36,  29, 23,  18, 15,
};

We can use this values to set the time-slice for a process p as:

time_slice_p = (weigth_p / sum_{i=0}^{n-1} weight_i) * sched_latency

and vruntime for a job i:

vruntime_i = vruntime_i + (weight_0 / weight_i) * runtime_i

The focus of CFS is efficiency, so it uses red-black-tree to keep the running
processes. If a process goes to sleep (waiting I/O..) the process is removed
from the tree. Processes are ordered in the tree by vruntime, and most
operations (such as insertion and deletion) are logarithmic in time O(logn).

There are other useful properties of CFS that we will not discuss.

