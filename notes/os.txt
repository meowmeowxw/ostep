What happens when a program runs?

-------------------------------------------------------------------------------
Too many things.. :(
-------------------------------------------------------------------------------

It does one very simple thing: it executes instruction. To do that the CPU
fetches an instruction from memmory, decodes it and exectues it. The CPU
repeats this cycle until the program completes.

However when a program runs, a lot of other things are going on.

There is a body of software, in fact, that is responsible for making it easy
to run programs, allowing programs to share memory, enabling programs to
interact with devices, and other fun stuff like that. This system is called
the operating system (OS).

The primary way the OS does this is through a general technique that we
call virtualization. That is, the OS takes a physical resource (such as
the processor, or memory, or a disk) and transforms it into a more general,
powerful, and easy--to-use virtual form of itself.

The OS also provides some interfaces (APIs) that you can call.  A typical OS,
in fact, exports a few hundred system calls that are available to applications.

Turning a single CPU (or small set of them) into a seemingly infinite number
of CPUs and thus allowing many programs to seemingly run at once is what we
call virtualizing the CPU.

Each process accesses its own private virtual address space, which the OS
somehow maps onto the physical memory of the machine.  A memory reference
within one running program does not affect the address space of other processes
(or the OS itself); as far as the running program is concerned, it has
physical memory all to itself (Virtualizing memory).  The reality, however,
is that physical memory is a shared resource, managed by the operating system.

-------------------------------------------------------------------------------
Process
-------------------------------------------------------------------------------

The process is the major OS abstraction of a running program. At any point
in time, the process can be described by its state: the contents of memory
in its address space, the contents of CPU registers (including the program
counter and stack pointer, among others), and information about I/O (such
as open files which can be read or written).

The process API consists of calls programs can make related to processes.
Typically, this includes creation, destruction, and other useful calls.

Processes exist in one of many different process states, including running,
ready to run, and blocked. Different events (e.g., getting scheduled or
descheduled, or waiting for an I/O to complete) transition a process from
one of these states to the other.

A process list contains information about all processes in the sys-- tem. Each
entry is found in what is sometimes called a process control block (PCB), which
is really just a structure that contains information about a specific process.

-------------------------------------------------------------------------------
Process API
-------------------------------------------------------------------------------

Each process has a name; in most systems, that name is a number known as a
process ID (PID).

The fork() system call is used in UNIX systems to create a new process.
The creator is called the parent; the newly created process is called the
child. As sometimes occurs in real life [J16], the child process is a nearly
identical copy of the parent.

The wait() system call allows a parent to wait for its child to complete
execution.

The exec() family of system calls allows a child to break free from its
similarity to its parent and execute an entirely new program.

A U NIX shell commonly uses fork(), wait(), and exec() to launch user
commands; the separation of fork and exec enables features like input/output
redirection, pipes, and other cool features, all without changing anything
about the programs being run.

Process control is available in the form of signals, which can cause jobs
to stop, continue, or even terminate.

Which processes can be controlled by a particular person is encapsulated in
the notion of a user; the operating system allows multiple users onto the
system, and ensures users can only control their own processes.

A superuser can control all processes (and indeed do many other things);
this role should be assumed infrequently and with caution for security reasons.

-------------------------------------------------------------------------------
Limited Direct Execution
-------------------------------------------------------------------------------

The CPU should support at least two modes of execution: a restricted user
mode and a privileged (non-restricted) kernel mode.

Typical user applications run in user mode (In this way the process can't
issue I/O directly to the disk or other privileged actions), and use a system
call to trap into the kernel to request operating system services.

The trap instruction saves register state carefully, changes the hardware
status to kernel mode, and jumps into the OS to a pre-specified destination:
the trap table.

When the OS finishes servicing a system call, it returns to the user program
via another special return-from-trap instruction, which reduces privilege
and returns control to the instruction after the trap that jumped into the OS.

The trap tables must be set up by the OS at boot time, and make sure that
they cannot be readily modified by user programs. All of this is part of
the limited direct execution protocol which runs programs efficiently but
without loss of OS control.

Once a program is running, the OS must use hardware mechanisms to ensure the
user program does not run forever, namely the timer interrupt. This approach
is a non-cooperative approach to CPU scheduling.

Sometimes the OS, during a timer interrupt or system call, might wish to
switch from running the current process to a different one, a low-level
technique known as a context switch. This decision is taken by the scheduler
using a scheduling policy.


-------------------------------------------------------------------------------
Scheduling
-------------------------------------------------------------------------------

The first thing we need to do is to introduce the assumptions about the
processes running in the system (workload). Possible workload assumptions:

1. Each job runs for the same amount of time.  2. All jobs arrive at the
same time.  3. Once started, each job runs to completion.  4. All jobs only
use the CPU (i.e., they perform no I/O) 5. The run-time of each job is known.

To measure how well a scheduling policy behave we need to define the scheduling
metric. The easiest metric is the turnaround time.  The turnaround time of
a job is defined as the time at which the job completes minus the time at
which the job arrived in the system:

T_turnaround = T_completion - T_arrival

------- FIFO -------

Let's start using FIFO (First-In-First-Out) policy.

Assume that a job completes in 10 seconds, and job A, B, C arrive in the
system at roughly the same time (T_arrival = 0).

Job A (0 -> 10) 
Job B (0 -> 20) 
Job C (0 -> 30)

In this case average T_turnaround = (10 + 20 + 30) / 3 = 20 s, which is a
good time.

Now let's relax assumption 1. and suppose that A runs 100 seconds, while B
and C for 10 seconds.

Job A (0 -> 100) 
Job B (0 -> 110) 
Job C (0 -> 120)

Average T_turnaround = (100 + 110 + 120) / 3 = 110 s, which is terrible.

------- SJF -------

SJF (Shortest Job First) run the shortest job first (wow), then the next
shortest, and so on. If we take the precedent case:

Job B (0 -> 10) 
Job C (0 -> 20) 
Job A (0 -> 120)

Average T_turnaround = (10 + 20 + 120) / 3 = 50 s which is way better
than FIFO.

Now let's relax assumption 2, and suppose that A arrives at t = 0, while B
and C arrives at t = 10 and both run for 10 seconds.

Job A (0 -> 100)
Job B (10 -> 110) 
Job C (10 -> 120)

Average T_turnaround = (100 + (110 - 10) + (120 - 10)) / 3 = 103.33 s.

------- STCF -------

TO address this concern, we need to relax assumption 3.

Using a STCF (Shortest Time to Completion First) policy the above case will
be scheduled as:

Job A (0 -> 10) 
Job B (10 -> 20) 
Job C (10 -> 30) 
Job A (30 -> 120)

Average T_turnaround = ((120 - 0) + (20 - 10) + (30 - 10)) / 3 = 50 s.

Thus, if we knew job lengths, and that jobs only used the CPU, and our
only metric was turnaround time, STCF would be a great policy. However, the
introduction of time-shared machines changed all that.  Now users would sit
at a terminal and demand interactive performance from the system as well. And
thus, a new metric was born: response time.

We define response time as the time from when job arrives to the system to
the first time is scheduled.

T_response = T_firstrun - T_arrival

If A has T_arrival = 0, B T_arrival = 10, C T_arrival = 10, and each job
lasts for 10 seconds then

Average T_response = (0 + 0 + 10) / 3 = 3.33 s

In this case STCF and SJF are not particularly good for response time.

------- RR -------

RR (Round Robin) don't wait for jobs to finish, instead it runs job for a
time slice and then switches to the next job in the run queue.

Assume A, B, C arrive at the same time in the system and each wish to run
for 3 seconds. If we set a time slice of 1 RR behave this way:

A (0 -> 1) 
B (1 -> 2) 
C (1 -> 3) 
A (3 -> 4) 
B (4 -> 5) 
C (5 -> 6) 
A (6 -> 7) 
B (7 -> 8) 
C (8 -> 9)

Average T_response = (0 + 1 + 2) / 3 = 1 s.

With SJF:

Average T_response = (0 + 3 + 6) / 3 = 3 s.

It sounds a good idea to set a very small time slice, however the cost of a
context switch is pretty big since the system must save/restore registers,
TLB, branch predictors and other on-chip hardware.

How good is turnaround time?

Average T_turnaround = (7 + 8 + 9) / 3 = 8 s which is awful.

We can see that there isn't a perfect policy for scheduling and we need to
trade the T_response for T_turnaround.

